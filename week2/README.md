# Intro to Statistics and Machine Learning

## Day 1
  * See the [Statistical Inference & Hypothesis Testing](Statistics%201.pptx) slides
  * Review the [sampling means Rmarkdown file](sampling%20means%20HW.Rmd) (preview the output [here](http://htmlpreview.github.io/?https://github.com/msr-ds3/coursework/blob/master/week2/sampling_means_HW.html))
  * Read Chapter 7 of [Introduction to Statistical Thinking (With R, Without Calculus)](http://pluto.huji.ac.il/~msby/StatThink/) for sampling distribution recap and Chapters 5 and 6 have terrific random variables recap and normal distribution recap as well.

<!--
  * Check out Chapters 7, 8, and 9 of [Introduction to Statistical Thinking (With R, Without Calculus)](http://pluto.huji.ac.il/~msby/StatThink/)
-->

## Day 2
  * See the [Prediction and Regression](Prediction%20and%20Regression.pptx) slides
  * HW2 within this week has a large set of problems where you're going to be able to learn all about Orange Juice!
  * See Chapter 3 of [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) on regression
  * Also covered in Chapter 14 of [Introduction to Statistical Thinking](http://pluto.huji.ac.il/~msby/StatThink/)

## Day 3
  * See the [Interpretation t statistics model fit](Interpretation%20t%20statistics%20model%20fit.pptx) slides
  * HW3 within this week investigates interpreting coefficients and cross validation.
  * Have a look at the [cross_validation_hack](cross_validation_hack.Rmd) which goes through K folds cross validation step by step.
  * See [here](http://home.wlu.edu/~gusej/econ398/notes/logRegressions.pdf) for more on log-log transformations in regression.
  * Chapter 12 of [Introduction to Statistical Thinking](http://pluto.huji.ac.il/~msby/StatThink/) covers hypothesis testing
  * See Chapter 5 of [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) on cross-validation

## Day 4
  * See the [logistic regression and regularization](logit.pptx)
  * HW4 looks at logistic regression for predicting who will take the SAT and LASSO for prediction on the OJ data
  * See Chapter 4 of [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) on classification and Chapter 6 on regularization

## Day 5
  * [Least squares](http://students.brown.edu/seeing-theory/regression/index.html#first) visualization
  * [Manual model fitting](https://jmhmsr.shinyapps.io/modelfit/) shiny app
  * [Gradient descent](http://htmlpreview.github.io/?https://github.com/jhofman/msd2017/blob/master/lectures/lecture_6/gradient_descent.html) animation
  * See [here](http://modelingsocialdata.org/lectures/2017/02/24/lecture-6-regression-1.html) for a table of complexity for model fitting and [here](https://github.com/jhofman/msd2017-notes/blob/master/lecture_6/lecture_6.pdf) for the gory details behind solving the normal equations and gradient descent
  * A notebook on [linear models](https://github.com/msr-ds3/coursework/blob/master/week2/linear_models.ipynb) with the `modelr` from the tidyverse
  * [Slides](https://www.slideshare.net/jakehofman/modeling-social-data-lecture-7-model-complexity-and-generalization), [notes](https://github.com/jhofman/msd2017-notes/blob/master/lecture_7/lecture_7.pdf) on evaluating models and overfitting
  * A [notebook](model_evaluation.ipynb) on model evaluation
  * A [notebook](complexity_control.ipynb) on overfitting and cross-validation
