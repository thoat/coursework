---
title: "HW4"
author: "Thoa Ta"
date: "June 22, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
library(boot)
library(dplyr)
library(plyr)
# library(doBy)
library(reshape2)
library(tidyr)
library(glmnet)

sat <- read.csv("sat.csv")
colnames(sat)
head(sat)
summary(sat)

# What percent of the sample took the SATs?
tookSAT <- sat %>% filter(satobs == 1) %>% nrow()
tookSAT <- tookSAT/nrow(sat)
tookSAT

nrow(filter(sat, satobs ==1))/nrow(sat)

# What is the average score rank of HS students who took the SAT versus those that didn't?
detach(package:plyr)
sat %>%
  group_by(satobs) %>%
  summarize(avg_HS_rank = mean(rank))
### satobs      avg_HS_rank
### <int>         <dbl>
### 0	50.97900	1	69.48088

# Build a model to predict who is going to take the SAT. Use everything (except some...) as a RHS variable 
mySat_OLS <- glm(data = sat, formula = satobs ~ rank + mcol + flhs + fcol + black + asian + nsib + vocab + rdsc + matsc + pict + lgsc + mosaic)
summary(mySat_OLS)

mySat_logit <- glm(data = sat, family = "binomial", formula = satobs ~ rank + mcol + flhs + fcol + black + asian + nsib + vocab + rdsc + matsc + pict + lgsc + mosaic)
summary(mySat_logit)

set.seed(17)
cv.error <- rep(0, 5)
for (i in 1:5) {
  cv.error[i] <- cv.glm(sat, mySat_OLS, K=5)$delta[1]
}
cv.error

fit <- mySat_logit
summary(fit) # display results 
confint(fit, level = 0.95)  # 95% CI for the coefficients 
exp(coef(fit)) # exponentiated coefficients
exp(confint(fit, level = 0.95)) # 95% CI for exponentiated coefficients 
predict(fit, type="response") # predicted values
residuals(fit, type="deviance") # residuals 

#####################
# Orange Juice data
#####################
# Run a LASSO model for the same cross-validated OLS model that gave you the lowest MSE.
ff <- logmove ~ log(price.thisWeek)*brand*feat + 
                log(price.thisWeek)*brand + log(price.lastWeek)*brand + 
                INCOME * brand +
                log(price.lastWeek)*AGE60 + AGE60*feat*INCOME +
                log(price.thisWeek)*EDUC*INCOME + 
                ETHNIC*feat*EDUC*INCOME + 
                log(price.lastWeek)*HHLARGE + log(price.thisWeek)*HHLARGE*feat +
                log(price.thisWeek)*WORKWOM*EDUC*INCOME + log(price.lastWeek)*WORKWOM*EDUC + 
                log(price.thisWeek)*HVAL150 + 
                log(price.thisWeek)*SSTRDIST*EDUC + 
                SSTRVOL*EDUC + CPDIST5*EDUC + 
                log(price.lastWeek)*CPWVOL5*EDUC
oj_matrix <- model.matrix(ff, df_with_lagged_prices)

# x has all the features i.e. explanatory variables
x <- oj_matrix
# y has the response variable
y <- as.matrix(df_with_lagged_prices$logmove)

# Left-most line is the CV'ed lambda. Other line is lambda one standard deviation from that; sometimes people use that to prevent overfitting even more (i.e. the bigger lambda makes the MSE a little bigger, but that prevents overfitting)
myLASSO_oj_cv <- cv.glmnet(x, y, alpha=1)
plot(myLASSO_oj_cv)

# The graph below shows how individual coefficients become significant over time when we decrease (???) the L1 penalty and the number of features which "turn on" as the penalty term decreases.

# as lambda increases (from R to L), coefficients come closer to 0. 
myLASSO_oj_model <- glmnet(x, y, alpha=1)
plot(myLASSO_oj_model)


coef(myLASSO_oj_cv, s="lambda.min")

myLASSO_oj_cv <- cv.glmnet(x, y, alpha=0.5)
coef(myLASSO_oj_cv, s="lambda.min")

# Now hold out 10% of Oj data for testing. The remaining 90% will be used to train your LASSO model.
spliter <- floor(0.1 * nrow(df_with_lagged_prices))

indices <- 1:nrow(df_with_lagged_prices)
set.seed(17)
split_index <- sample(indices, spliter)
test_set <- df_with_lagged_prices[split_index, ]
train_set <- df_with_lagged_prices[-split_index, ]

oj_matrix <- model.matrix(ff, train_set)

# x has all the features i.e. explanatory variables
x <- oj_matrix
# y has the response variable
y <- as.matrix(train_set$logmove)
myLASSO_oj_model_2 <- glmnet(x, y, alpha=1)
myLASSO_oj_cv_2 <- cv.glmnet(x, y, alpha=1)

mse_hat <- predict(myLASSO_oj_cv_2, newx = model.matrix(ff, test_set), s="lambda.min") #????
mean(mse_hat)


```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
